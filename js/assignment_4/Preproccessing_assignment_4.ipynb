{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee81df1d-9b7d-403f-85db-ef4984af2a3e",
   "metadata": {},
   "source": [
    "# Yazan Aakel - 5171606\n",
    "## Data Visualization Course\n",
    "* this notebook is for manipulating the data to produce it in a suitable form for drawing the required charts and it's used in assignment 4 - maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0250634-4a9e-47a2-be97-5ffa24218557",
   "metadata": {},
   "source": [
    "# This is for dot density maps, producing trees names along with longitude and latitude for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a48baf2f-9437-4667-9e93-41c7f7474d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing .. Milwaukee_Final_2022-06-18.csv\n",
      "processing .. Anaheim_Final_2022-06-18.csv\n",
      "processing .. Irvine_Final_2022-06-18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rw/_n4qxq71007bwjr792f8v79w0000gn/T/ipykernel_2628/1956636864.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  aggregated_data = pd.concat([aggregated_data, data[['scientific_name', 'longitude_coordinate', 'latitude_coordinate', 'state']]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing .. RanchoCucamonga_Final_2022-06-18.csv\n",
      "processing .. Arlington_Final_2022-06-18.csv\n",
      "processing .. Rochester_Final_2022-06-18.csv\n",
      "processing .. Miami_Final_2022-06-18.csv\n",
      "processing .. Austin_Final_2022-06-18.csv\n",
      "processing .. Stockton_Final_2022-06-18.csv\n",
      "processing .. Seattle_Final_2022-06-18.csv\n",
      "processing .. LosAngeles_Final_2022-06-18.csv\n",
      "processing .. AuroraCO_Final_2022-06-18.csv\n",
      "processing .. NewYork_Final_2022-06-18.csv\n",
      "processing .. Oakland_Final_2022-06-18.csv\n",
      "processing .. SanFrancisco_Final_2022-06-18.csv\n",
      "processing .. Boston_Final_2022-06-18.csv\n",
      "processing .. Ontario_Final_2022-06-18.csv\n",
      "processing .. GrandRapids_Final_2022-06-18.csv\n",
      "processing .. Baltimore_Final_2022-06-18.csv\n",
      "processing .. Albuquerque_Final_2022-06-18.csv\n",
      "processing .. Greensboro_Final_2022-06-18.csv\n",
      "processing .. OklahomaCity_Final_2022-06-18.csv\n",
      "processing .. HuntingtonBeach_Final_2022-06-18.csv\n",
      "processing .. Fresno_Final_2022-06-18.csv\n",
      "processing .. Orlando_Final_2022-06-18.csv\n",
      "processing .. Nashville_Final_2022-06-18.csv\n",
      "processing .. StLouis_Final_2022-06-18.csv\n",
      "processing .. Tampa_Final_2022-06-18.csv\n",
      "processing .. LasVegas_Final_2022-06-18.csv\n",
      "processing .. Phoenix_Final_2022-06-18.csv\n",
      "processing .. Minneapolis_Final_2022-06-18.csv\n",
      "processing .. Indianapolis_Final_2022-06-18.csv\n",
      "processing .. NewOrleans_Final_2022-06-18.csv\n",
      "processing .. Madison_Final_2022-06-18.csv\n",
      "processing .. Columbus_Final_2022-06-18.csv\n",
      "processing .. Knoxville_Final_2022-06-18.csv\n",
      "processing .. GardenGrove_Final_2022-06-18.csv\n",
      "processing .. Honolulu_Final_2022-06-18.csv\n",
      "processing .. Plano_Final_2022-06-18.csv\n",
      "processing .. Richmond_Final_2022-06-18.csv\n",
      "processing .. Jerseycity_Final_2022-06-18.csv\n",
      "processing .. Detroit_Final_2022-06-18.csv\n",
      "processing .. Portland_Final_2022-06-18.csv\n",
      "processing .. Providence_Final_2022-06-18.csv\n",
      "processing .. SanJose_Final_2022-06-18.csv\n",
      "processing .. Atlanta_Final_2022-06-18.csv\n",
      "processing .. ColoradoSprings_Final_2022-06-18.csv\n",
      "processing .. Pittsburgh_Final_2022-06-18.csv\n",
      "processing .. Louisville_Final_2022-06-18.csv\n",
      "processing .. CapeCoral_Final_2022-06-18.csv\n",
      "processing .. Denver_Final_2022-06-18.csv\n",
      "processing .. Worcester_Final_2022-06-18.csv\n",
      "processing .. OverlandPark_Final_2022-06-18.csv\n",
      "processing .. Houston_Final_2022-06-18.csv\n",
      "processing .. Durham_Final_2022-06-18.csv\n",
      "processing .. SanDiego_Final_2022-06-18.csv\n",
      "processing .. Dallas_Final_2022-06-18.csv\n",
      "processing .. SantaRosa_Final_2022-06-18.csv\n",
      "processing .. DesMoines_Final_2022-06-18.csv\n",
      "processing .. Sacramento_Final_2022-06-18.csv\n",
      "processing .. WashingtonDC_Final_2022-06-18.csv\n",
      "processing .. Buffalo_Final_2022-06-18.csv\n",
      "processing .. SiouxFalls_Final_2022-06-18.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#this function is to get each tree name along with its longitude and latitude\n",
    "def process_data():\n",
    "    # Directory where your CSV files are located\n",
    "    data_directory = \"./Desktop/data-vis\"\n",
    "\n",
    "    # Create a directory for the output CSV file\n",
    "    output_directory = os.path.join(data_directory, \"output\")\n",
    "    os.makedirs(output_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Initialize an empty dataframe to store the aggregated data\n",
    "    aggregated_data = pd.DataFrame(columns=[\"scientific_name\", \"longitude_coordinate\", \"latitude_coordinate\", \"state\"])\n",
    "\n",
    "    # List all CSV files in the directory\n",
    "    csv_files = [file for file in os.listdir(data_directory) if file.endswith(\".csv\")]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(\"processing .. \" + csv_file)\n",
    "\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(os.path.join(data_directory, csv_file), usecols=['scientific_name', 'longitude_coordinate', 'latitude_coordinate',  'state'])\n",
    "        data = data.dropna(subset=['scientific_name', 'longitude_coordinate', 'latitude_coordinate'])\n",
    "\n",
    "        # Drop duplicates based on scientific_name\n",
    "        data = data.drop_duplicates(subset='scientific_name', keep='first')\n",
    "\n",
    "        # Add scientific_name, longitude, and latitude to the aggregated_data dataframe\n",
    "        aggregated_data = pd.concat([aggregated_data, data[['scientific_name', 'longitude_coordinate', 'latitude_coordinate', 'state']]], ignore_index=True)\n",
    "\n",
    "    output_file = os.path.join(output_directory, \"scientific_name_coordinates.csv\")\n",
    "    aggregated_data.to_csv(output_file, index=False)\n",
    "\n",
    "process_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996ed67-c36a-45da-b4b5-b6496e318e03",
   "metadata": {},
   "source": [
    "# This is for colorpleth map task, producing each state with total number of trees for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039724e8-1e70-4e62-a463-1a7e9bdf00e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing .. Milwaukee_Final_2022-06-18.csv\n",
      "processing .. Anaheim_Final_2022-06-18.csv\n",
      "processing .. Irvine_Final_2022-06-18.csv\n",
      "processing .. RanchoCucamonga_Final_2022-06-18.csv\n",
      "processing .. Arlington_Final_2022-06-18.csv\n",
      "processing .. Rochester_Final_2022-06-18.csv\n",
      "processing .. Miami_Final_2022-06-18.csv\n",
      "processing .. Austin_Final_2022-06-18.csv\n",
      "processing .. Stockton_Final_2022-06-18.csv\n",
      "processing .. Seattle_Final_2022-06-18.csv\n",
      "processing .. LosAngeles_Final_2022-06-18.csv\n",
      "processing .. AuroraCO_Final_2022-06-18.csv\n",
      "processing .. NewYork_Final_2022-06-18.csv\n",
      "processing .. Oakland_Final_2022-06-18.csv\n",
      "processing .. SanFrancisco_Final_2022-06-18.csv\n",
      "processing .. Boston_Final_2022-06-18.csv\n",
      "processing .. Ontario_Final_2022-06-18.csv\n",
      "processing .. GrandRapids_Final_2022-06-18.csv\n",
      "processing .. Baltimore_Final_2022-06-18.csv\n",
      "processing .. Albuquerque_Final_2022-06-18.csv\n",
      "processing .. Greensboro_Final_2022-06-18.csv\n",
      "processing .. OklahomaCity_Final_2022-06-18.csv\n",
      "processing .. HuntingtonBeach_Final_2022-06-18.csv\n",
      "processing .. Fresno_Final_2022-06-18.csv\n",
      "processing .. Orlando_Final_2022-06-18.csv\n",
      "processing .. Nashville_Final_2022-06-18.csv\n",
      "processing .. StLouis_Final_2022-06-18.csv\n",
      "processing .. Tampa_Final_2022-06-18.csv\n",
      "processing .. LasVegas_Final_2022-06-18.csv\n",
      "processing .. Phoenix_Final_2022-06-18.csv\n",
      "processing .. Minneapolis_Final_2022-06-18.csv\n",
      "processing .. Indianapolis_Final_2022-06-18.csv\n",
      "processing .. NewOrleans_Final_2022-06-18.csv\n",
      "processing .. Madison_Final_2022-06-18.csv\n",
      "processing .. Columbus_Final_2022-06-18.csv\n",
      "processing .. Knoxville_Final_2022-06-18.csv\n",
      "processing .. GardenGrove_Final_2022-06-18.csv\n",
      "processing .. Honolulu_Final_2022-06-18.csv\n",
      "processing .. Plano_Final_2022-06-18.csv\n",
      "processing .. Richmond_Final_2022-06-18.csv\n",
      "processing .. Jerseycity_Final_2022-06-18.csv\n",
      "processing .. Detroit_Final_2022-06-18.csv\n",
      "processing .. Portland_Final_2022-06-18.csv\n",
      "processing .. Providence_Final_2022-06-18.csv\n",
      "processing .. SanJose_Final_2022-06-18.csv\n",
      "processing .. Atlanta_Final_2022-06-18.csv\n",
      "processing .. ColoradoSprings_Final_2022-06-18.csv\n",
      "processing .. Pittsburgh_Final_2022-06-18.csv\n",
      "processing .. Louisville_Final_2022-06-18.csv\n",
      "processing .. CapeCoral_Final_2022-06-18.csv\n",
      "processing .. Denver_Final_2022-06-18.csv\n",
      "processing .. Worcester_Final_2022-06-18.csv\n",
      "processing .. OverlandPark_Final_2022-06-18.csv\n",
      "processing .. Houston_Final_2022-06-18.csv\n",
      "processing .. Durham_Final_2022-06-18.csv\n",
      "processing .. SanDiego_Final_2022-06-18.csv\n",
      "processing .. Dallas_Final_2022-06-18.csv\n",
      "processing .. SantaRosa_Final_2022-06-18.csv\n",
      "processing .. DesMoines_Final_2022-06-18.csv\n",
      "processing .. Sacramento_Final_2022-06-18.csv\n",
      "processing .. WashingtonDC_Final_2022-06-18.csv\n",
      "processing .. Buffalo_Final_2022-06-18.csv\n",
      "processing .. SiouxFalls_Final_2022-06-18.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "state_areas = {\n",
    "    'Alabama': 135767,\n",
    "    'Alaska': 1717855,\n",
    "    'Arizona': 295234,\n",
    "    'Arkansas': 137733,\n",
    "    'California': 423968,\n",
    "    'Colorado': 269601,\n",
    "    'Connecticut': 12534,\n",
    "    'Delaware': 5047,\n",
    "    'District of Columbia': 177,\n",
    "    'Florida': 170304,\n",
    "    'Georgia': 148958,\n",
    "    'Hawaii': 16637,\n",
    "    'Idaho': 216443,\n",
    "    'Illinois': 144774,\n",
    "    'Indiana': 92789,\n",
    "    'Iowa': 145746,\n",
    "    'Kansas': 211754,\n",
    "    'Kentucky': 102908,\n",
    "    'Louisiana': 135659,\n",
    "    'Maine': 91646,\n",
    "    'Maryland': 25313,\n",
    "    'Massachusetts': 20203,\n",
    "    'Michigan': 250488,\n",
    "    'Minnesota': 225163,\n",
    "    'Mississippi': 120383,\n",
    "    'Missouri': 180560,\n",
    "    'Montana': 380832,\n",
    "    'Nebraska': 199097,\n",
    "    'Nevada': 286380,\n",
    "    'New Hampshire': 23186,\n",
    "    'New Jersey': 19050,\n",
    "    'New Mexico': 314917,\n",
    "    'New York': 141297,\n",
    "    'North Carolina': 139390,\n",
    "    'North Dakota': 178712,\n",
    "    'Ohio': 116096,\n",
    "    'Oklahoma': 177846,\n",
    "    'Oregon': 248608,\n",
    "    'Pennsylvania': 119283,\n",
    "    'Rhode Island': 2678,\n",
    "    'South Carolina': 82931,\n",
    "    'South Dakota': 199729,\n",
    "    'Tennessee': 109153,\n",
    "    'Texas': 695662,\n",
    "    'Utah': 219882,\n",
    "    'Vermont': 24906,\n",
    "    'Virginia': 110787,\n",
    "    'Washington': 184666,\n",
    "    'West Virginia': 62259,\n",
    "    'Wisconsin': 140268,\n",
    "    'Wyoming': 253335\n",
    "}\n",
    "\n",
    "def process_data():\n",
    "    # Directory where your CSV files are located\n",
    "    data_directory = \"./Desktop/data-vis\"\n",
    "\n",
    "    # Create a directory for the output CSV file\n",
    "    output_directory = os.path.join(data_directory, \"output\")\n",
    "    os.makedirs(output_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Initialize an empty dataframe to store the aggregated data\n",
    "    aggregated_data = pd.DataFrame(columns=[\"state\", \"total_tree_count\"])\n",
    "\n",
    "    # List all CSV files in the directory\n",
    "    csv_files = [file for file in os.listdir(data_directory) if file.endswith(\".csv\")]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(\"processing .. \" + csv_file)\n",
    "\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(os.path.join(data_directory, csv_file), usecols=['state', 'scientific_name'])\n",
    "\n",
    "        # Group the data by state and scientific_name and count the occurrences for each state\n",
    "        state_tree_count = data.groupby(['state', 'scientific_name']).size().reset_index(name=\"tree_count\")\n",
    "        #state_tree_count = state_tree_count.groupby('state')['tree_count'].sum().reset_index(name=\"total_tree_count\")\n",
    "\n",
    "        # Add state-specific data to the aggregated_data dataframe\n",
    "        aggregated_data = pd.concat([aggregated_data, state_tree_count], ignore_index=True)\n",
    "\n",
    "    # Group the aggregated_data dataframe to get unique total tree counts for each state\n",
    "    aggregated_data = aggregated_data.groupby('state')['tree_count'].sum().reset_index(name=\"total_tree_count\")\n",
    "    areas_df = pd.DataFrame(list(state_areas.items()), columns=['state', 'area'])\n",
    "    aggregated_data = pd.merge(aggregated_data, areas_df, on='state', how='left')\n",
    "\n",
    "    # Sort the aggregated_data dataframe by total_tree_count in descending order\n",
    "    aggregated_data.sort_values(by=[\"total_tree_count\"], inplace=True, ascending=False)\n",
    "\n",
    "    # Save the aggregated data to a CSV file\n",
    "    output_file = os.path.join(output_directory, \"state_total_tree_counts.csv\")\n",
    "    aggregated_data.to_csv(output_file, index=False)\n",
    "\n",
    "process_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65184f-3b7a-4f38-acf7-877a5c63ceff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
